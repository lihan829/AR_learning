{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import re\n",
    "\n",
    "# Phonology sets\n",
    "h_tone = set(\"áéíóú\")\n",
    "l_tone = set(\"àèìòù\")\n",
    "f_tone = set(\"âêîôû\")\n",
    "r_tone = set(\"ǎěǐǒǔ\")\n",
    "untoned = set(\"aeiou\")  # For long vowels (â indicates a short vowel with fall tone; âa a long F)\n",
    "vowels = h_tone | l_tone | r_tone | f_tone | untoned\n",
    "tones = h_tone | l_tone  # Combine high and low tones\n",
    "special_tones = r_tone | f_tone  # Special tones (F, R)\n",
    "\n",
    "max_syl_weight = 3 #default\n",
    "tone_bearing_unit = 0 # 0: syllable 1:mora\n",
    "moraic_coda = 1  # 1 if coda carries mora, else 0\n",
    "word_edge = 1 #1 if there are word boundaries; 0 no boundary\n",
    "\n",
    "\n",
    "\n",
    "class Autorep:\n",
    "\n",
    "    def __init__(self, word='', ocp_mel='', assoc=None, boundary=0):\n",
    "        \"\"\"\n",
    "        Initialize an Autorep object.\n",
    "\n",
    "        Parameters:\n",
    "        - word (str): The word with tone markers.\n",
    "        - tone (str): The tone markers directly extracted from the word (HFLR).\n",
    "        - mel (str): The melody (F -> HL and R -> LH) before OCP.\n",
    "        - ocp_mel (str): The OCP-applied tone representation of the word.\n",
    "        - assoc (list): A list of tuples (j, k) indicating the association \n",
    "                        between tone (indexed by j), mora (indexed by i), \n",
    "                        and syllable (indexed by k).\n",
    "        \"\"\"\n",
    "        self.word = word\n",
    "        self.tone = \"\"\n",
    "        self.mel = \"\"\n",
    "        self.ocp_mel = ocp_mel\n",
    "        self.assoc = self.sort_assoc(assoc if assoc is not None else [])\n",
    "    \n",
    "        self.boundary = boundary\n",
    "\n",
    "        self.tone_labels = {\"H\": h_tone, \"L\": l_tone, \"F\": f_tone, \"R\": r_tone}\n",
    "        \n",
    "        \n",
    "        if self.word:\n",
    "            self._process_word()\n",
    "        \n",
    "        if self.boundary == 1:\n",
    "            self.ocp_mel_wb = self._wrap()[0]\n",
    "            self.boundary = self._wrap()[1]\n",
    "        \n",
    "        self.syl_list = [i + 1 for i in range(self.get_max(\"s\"))]\n",
    "        self.moralist = [max((tup[1] for tup in self.assoc if tup[-1] == j), default=0) for j in self.syl_list\n",
    "                         ]\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "    # def _wrap(self,num = 1):\n",
    "    #     self.ocp_mel_wb = '<' + self.ocp_mel + '>'\n",
    "        \n",
    "    #     # Create a set to store word boundaries\n",
    "    #     self.boundary = set()\n",
    "        \n",
    "    #     if self.ocp_mel:\n",
    "    #     # Check beginning of the word\n",
    "    #         if self.ocp_mel[0] == \"H\":\n",
    "    #             self.boundary.add(\"<H\")\n",
    "    #         elif self.ocp_mel[0] == \"L\":\n",
    "    #             self.boundary.add(\"<L\")\n",
    "\n",
    "    #         # Check end of the word\n",
    "    #         if self.ocp_mel[-1] == \"H\":\n",
    "    #             self.boundary.add(\"H>\")\n",
    "    #         elif self.ocp_mel[-1] == \"L\":\n",
    "    #             self.boundary.add(\"L>\")\n",
    "        \n",
    "    #     return self.ocp_mel_wb, self.boundary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "    def _process_word(self):\n",
    "        \"\"\"Process the word to extract tones, assign associations, and apply OCP.\"\"\"\n",
    "        syllables = self.word.split(\".\")\n",
    "        self.tone = \"\".join(\n",
    "            next((k for k, v in self.tone_labels.items() if seg in v), \"\") \n",
    "            for seg in self.word\n",
    "        )\n",
    "\n",
    "        if len(syllables) == len(self.tone):\n",
    "            for i, syl in enumerate(syllables):\n",
    "                syl_weight = self.check_coda(syl) + self.vowel_count(syl)\n",
    "                ### if wants do mora bering \n",
    "                # for j in range(syl_weight):\n",
    "                #     self.assoc.append((self.tone[i], j + 1, i + 1))\n",
    "                self.assoc.append((self.tone[i], syl_weight, i + 1))\n",
    "                \n",
    "        # else:\n",
    "        #     print(f\"something seems of {self.show()}\")\n",
    "       \n",
    "        self._flatten_tones(syllables)  # convert F and R into HL and LH\n",
    "        self.mel = \"\".join(tone for tone, _, _ in self.assoc)\n",
    "        self.ocp_mel = re.sub(r\"(.)\\1+\", r\"\\1\", self.mel)\n",
    "        self._update_tone_indices()\n",
    "        \n",
    "        \n",
    "\n",
    "    def _flatten_tones(self, syllables):\n",
    "        # print(self.assoc)\n",
    "        tone_map = {\"F\": (\"H\", \"L\"), \"R\": (\"L\", \"H\")}\n",
    "        for tone, (t1, t2) in tone_map.items():\n",
    "            # print(tone, (t1, t2))\n",
    "            if tone in self.tone:\n",
    "                for i, (t, m, s) in enumerate(self.assoc):\n",
    "                        # print(t, m, s)\n",
    "                        if t == tone:\n",
    "                            self.assoc[i] = (t1,m,s)\n",
    "                            self.assoc.insert(i+1,(t2,m,s))\n",
    "        \n",
    "        \n",
    "    def _update_tone_indices(self):\n",
    "        \"\"\"Update tone indices in association list to match OCP melody.\"\"\"\n",
    "        j, i = 0, 0\n",
    "        while j < len(self.assoc) and i < len(self.ocp_mel):\n",
    "            if self.assoc[j][0] == self.ocp_mel[i]:\n",
    "                t, m, s = self.assoc[j]\n",
    "                self.assoc[j] = (i + 1, m, s)  # Update tone index\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "\n",
    "\n",
    "    def get_max(self, target):\n",
    "        \"\"\"\n",
    "        self.get_max('t') returns the biggest indexed tone\n",
    "        self.get_max('s') returns the biggest indexed syllable\n",
    "        self.get_max('m') returns the total moras\n",
    "        \"\"\"\n",
    "        index_map = {'t': 0, 's': 2}\n",
    "\n",
    "        if target in index_map:\n",
    "            return max(\n",
    "                (item[index_map[target]] for item in self.assoc if item[index_map[target]] is not None), \n",
    "                default=0\n",
    "            )\n",
    "        elif target == 'm':\n",
    "            return sum(self.moralist)  # Return the sum of self.moralist if target is 'm'\n",
    "\n",
    "        return 0  # Return 0 for invalid target\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_coda(syl):\n",
    "        \"\"\"Check if a syllable contains a coda.\"\"\"\n",
    "        for i in range(1, len(syl)):\n",
    "            if syl[i] not in vowels and syl[i - 1] in vowels:\n",
    "                return moraic_coda\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def vowel_count(syl):\n",
    "        \"\"\"Count the number of vowels and adjust for special tones.\"\"\"\n",
    "        count = 0\n",
    "        for i, char in enumerate(syl):\n",
    "            if char in vowels or char in tones:\n",
    "                count += 1\n",
    "            elif (\n",
    "                char in special_tones\n",
    "                and i + 1 < len(syl)\n",
    "                and syl[i + 1] not in vowels\n",
    "            ):\n",
    "                count += 2\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def mora_count(string):\n",
    "        \"\"\"Count the number of mora in a string.\"\"\"\n",
    "        mora_count = 0\n",
    "        mora_list = []\n",
    "        syllables = string.split(\".\")\n",
    "        for syl in syllables:\n",
    "            syl_weight = Autorep.check_coda(syl) + Autorep.vowel_count(syl)\n",
    "            mora_list.append(syl_weight)\n",
    "            mora_count += syl_weight\n",
    "        return mora_count, mora_list                                \n",
    "    \n",
    "    @staticmethod\n",
    "    def contour_count(s):\n",
    "        \"\"\"Count the number of contour tones in a string.\"\"\"\n",
    "        return sum(1 for char in s if char in special_tones)\n",
    "\n",
    "    @staticmethod\n",
    "    def index_reset(lst):   \n",
    "\n",
    "        \"\"\"Reset indices of the association list to start from 1.\"\"\"\n",
    "        if not lst:\n",
    "            return []\n",
    "        \n",
    "        else:\n",
    "            t_shift = min((t for (t, _, _) in lst if t is not None), default=0)\n",
    "            # m_shift = (m for (_, m, _ )in lst if m is not None)\n",
    "            s_shift = min((s for( _, _, s) in lst if s is not None), default=0)\n",
    "\n",
    "\n",
    "            return [\n",
    "                (\n",
    "                    (t - t_shift + 1) if t else None,\n",
    "                    m,\n",
    "                    (s - s_shift + 1) if s else None,\n",
    "                )\n",
    "                for (t, m, s) in lst\n",
    "            ]\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_assoc(assoc):\n",
    "        def custom_compare(x):\n",
    "            return float('inf') if x is None else x\n",
    "\n",
    "        sorted_assoc = sorted(\n",
    "            assoc, \n",
    "            key=lambda x: custom_compare(x[0]) if x[0] is not None \n",
    "                 else custom_compare(x[2]) if x[2] is not None \n",
    "                 else custom_compare(x[1]))\n",
    "        return sorted_assoc\n",
    "        \n",
    "        \n",
    "    def check_empty(self):\n",
    "        \"\"\"Check if the object is empty.\"\"\"\n",
    "        return not (self.word or self.assoc or self.mel or self.ocp_mel)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_modified_substring(list_a, list_b):\n",
    "        # print(list_a,list_b)\n",
    "        n, m = len(list_a), len(list_b)\n",
    "        \n",
    "        for i in range(m - n + 1):  # Slide through `list_b`\n",
    "            # Check if the first and last elements of `list_a` match the conditions\n",
    "            if list_a[0] <= list_b[i] and list_a[-1] <= list_b[i + n - 1]:  \n",
    "                # Check if the middle elements match exactly\n",
    "                if list_a[1:-1] == list_b[i+1:i+n-1]:  \n",
    "                    \n",
    "                    return True  \n",
    "                    \n",
    "        return False\n",
    "    \n",
    "\n",
    "    \n",
    "    def check_contain(container, containee):\n",
    "        # print(f\"if {container} contains {containee}\")\n",
    "        \n",
    "                \n",
    "        if not containee.moralist:\n",
    "            # print(f\"no moralist and meldoy {containee.ocp_mel_wb} in {container.ocp_mel_wb} match: {containee.ocp_mel_wb in container.ocp_mel_wb}\")\n",
    "            return containee.ocp_mel in container.ocp_mel  # If no syllables, check melody\n",
    "        \n",
    "        if not containee.ocp_mel:\n",
    "            # print(f\"{containee} has no melody\")\n",
    "            return containee.is_modified_substring(containee.moralist,container.moralist)\n",
    "\n",
    "        \n",
    "        \n",
    "        conditions = [\n",
    "            (container.check_empty(), \"container is empty\"),\n",
    "            (containee.moralist and not container.moralist, \"containee.moralist exists, but container.moralist is missing.\"),\n",
    "            (containee.ocp_mel and not container.ocp_mel, \"containee.ocp_mel exists, but container.ocp_mel is missing.\"),\n",
    "           # (not containee.boundary.issubset(container.boundary), \n",
    "            #    f\"containee.boundary ({containee.boundary}) is not a subset of container.boundary ({container.boundary}).\"),\n",
    "            (containee.count_full_tuples() > container.count_full_tuples(), \n",
    "                f\"containee.count_full_tuples() ({containee.count_full_tuples()}) > container.count_full_tuples() ({container.count_full_tuples()}).\"),\n",
    "            (containee.get_max('t') > container.get_max('t'), \n",
    "                f\"containee.get_max('t') ({containee.get_max('t')}) > container.get_max('t') ({container.get_max('t')}).\"),\n",
    "            (containee.get_max('s') > container.get_max('s'), \n",
    "                f\"containee.get_max('s') ({containee.get_max('s')}) > container.get_max('s') ({container.get_max('s')}).\"),\n",
    "            (containee.get_max('m') > container.get_max('m'), \n",
    "                f\"containee.get_max('m') ({containee.get_max('m')}) > container.get_max('m') ({container.get_max('m')}).\"),\n",
    "            (containee.count_full_tuples()>container.count_full_tuples(), f\"{container} more association\")\n",
    "        ]\n",
    "\n",
    "        # Check which conditions failed and print debug messages\n",
    "        failed_conditions = [msg for cond, msg in conditions if cond]\n",
    "\n",
    "        if failed_conditions:\n",
    "            # for msg in failed_conditions:\n",
    "            #     print(f\"❌ Debug: {msg}\")\n",
    "            return False\n",
    "\n",
    "        if containee == container:\n",
    "            return True\n",
    "        \n",
    "        if containee.check_empty():\n",
    "            return True  # `containee` is empty → always contained\n",
    "\n",
    "        if container in containee.next_ar():\n",
    "            return True\n",
    "    \n",
    "\n",
    "            \n",
    "        if containee.count_full_tuples()==0:\n",
    "            # print('no fully connected pair')\n",
    "            return containee.is_modified_substring(containee.moralist,container.moralist) #and containee.ocp_mel_wb in container.ocp_mel_wb\n",
    "        \n",
    "        \n",
    "        \n",
    "        match_positions = [m.start() for m in re.finditer(f\"(?={re.escape(containee.ocp_mel)})\", container.ocp_mel)]\n",
    "\n",
    "        if match_positions and container.boundary == 0:\n",
    "            # print(match_positions)\n",
    "            for match_pos in match_positions:\n",
    "                match = True\n",
    "                ## taking a restriction out of larger ar\n",
    "                \n",
    "                tone_idx_spanning_container = range(match_pos+1,match_pos+containee.get_max('t')+1)\n",
    "                # print(tone_idx_spanning_container)\n",
    "                # syl_start_idx_container = min(tup[2] for tup in container.assoc if tup[0] == match_pos + 1)\n",
    "                # syl_end_dex_container = syl_start_idx_container + containee.get_max('s')\n",
    "                # print(range(syl_start_idx_container,syl_end_dex_container + 1))\n",
    "                restriction_assoc = [tup for tup in container.assoc if\n",
    "                                     tup [0] in tone_idx_spanning_container or tup[0] is None]\n",
    "                                    #  tup [2] in range(syl_start_idx_container,syl_end_dex_container + 1)]\n",
    "                \n",
    "                # print(restriction_assoc)\n",
    "                restriction_ar = Autorep(\"\",containee.ocp_mel, containee.index_reset(restriction_assoc))\n",
    "                # print(restriction_ar)\n",
    "                \n",
    "                # print(f\"tone-syl list for containee {containee.tone_syl_list()}\")\n",
    "                # print(f\"tone-syl list for container {restriction_ar.tone_syl_list()}\")\n",
    "                # print(f\"syl_tone_listfor containee {containee.syl_tone_list()}\")\n",
    "                # print(f\"syl_tone_list for container {restriction_ar.syl_tone_list()}\")\n",
    "                if containee.is_modified_substring(containee.tone_syl_list(),restriction_ar.tone_syl_list()):\n",
    "                    if containee.is_modified_substring(containee.syl_tone_list(),restriction_ar.syl_tone_list()):\n",
    "                        for i in range(0,containee.get_max('s')):\n",
    "                            i_syl_weight_containee = max((tup[1] for tup in containee.assoc if tup[-1] == i+1), default=0)\n",
    "                            i_syl_weight_res = max((tup[1] for tup in restriction_ar.assoc if tup[-1] == i+1), default=0)\n",
    "                            # print(f\"in containee, the {i+1} syllable has weight of {max((tup[1] for tup in containee.assoc if tup[-1] == i+1), default=0)}\")\n",
    "                            # print(f\"in res, the {i+1} syllable has weight of {max((tup[1] for tup in restriction_ar.assoc if tup[-1] == i+1), default=0)}\")\n",
    "                            if i_syl_weight_containee != i_syl_weight_res:\n",
    "                                match = False\n",
    "                                break\n",
    "                        return match\n",
    "                \n",
    "            return False\n",
    "                        \n",
    "        # if container.boundary == 1 and containee.boundary == 1:\n",
    "        #     print(\"adding wv\")\n",
    "        #     if not container.ocp_mel_wb == containee.ocp_mel_wb:\n",
    "        #         return False\n",
    "        #     if containee.is_modified_substring(containee.tone_syl_list(),restriction_ar.tone_syl_list()):\n",
    "        #         if containee.is_modified_substring(containee.syl_tone_list(),restriction_ar.syl_tone_list()):\n",
    "        #             return True\n",
    "            \n",
    "            \n",
    "            \n",
    "        return False\n",
    "    \n",
    "                        \n",
    "\n",
    "    \n",
    "    def add_tone(self):\n",
    "        \"\"\"\n",
    "        Add an unassociated tone in the AR by updating the melody and the association list.\n",
    "        \n",
    "        - A new tone ('H' or 'L') is added to the melody.\n",
    "        - A new association (j, None, None) is added, where:\n",
    "            - j is one-unit higher than the previous tone's number or 1 if starting fresh.\n",
    "            - 'None' indicates the syllable is not associated with any tone unit.\n",
    "        \"\"\"\n",
    "        # Copy the existing associations to avoid modifying the original\n",
    "       \n",
    "        new_assoc = self.assoc.copy()\n",
    "        # Determine the next tone to add\n",
    "        if not self.ocp_mel:  # Empty string case\n",
    "            return [Autorep(ocp_mel='H', assoc=new_assoc + [(1, None, None)]),\n",
    "                    Autorep(ocp_mel='L', assoc=new_assoc + [(1, None, None)])]\n",
    "        \n",
    "        else:\n",
    "            # print('adding a tone')\n",
    "            next_tone = 'H' if self.ocp_mel[-1] == 'L' else 'L'\n",
    "            next_tone_index = self.get_max('t') + 1\n",
    "            \n",
    "            # Create the updated autorep\n",
    "            return [Autorep(\n",
    "                ocp_mel=self.ocp_mel + next_tone,\n",
    "                assoc=new_assoc + [(next_tone_index, None, None)]\n",
    "            )]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def add_syl(self,weight):\n",
    "        new_assoc = self.assoc.copy()\n",
    "        next_syl_index = self.get_max('s') + 1 if self.get_max('s') else 1\n",
    "        # for i in range(weight):\n",
    "        #     new_assoc.append((None, i+1, next_syl_index))\n",
    "        new_assoc.append((None, weight, next_syl_index))\n",
    "        \n",
    "        new_ar = Autorep(ocp_mel= self.ocp_mel,assoc = new_assoc)\n",
    "        return new_ar\n",
    "    \n",
    "\n",
    "            \n",
    "    def add_assoc(self):\n",
    "        floating_tone = min((t for (t, m, s) in self.assoc if not s), default=None)\n",
    "        floating_syl = min((s for (t, m, s) in self.assoc if not t), default=None)\n",
    "        valid_connected = [(t, m, s) for (t, m, s) in self.assoc if t and m and s]\n",
    "\n",
    "        if floating_tone is None and floating_syl is None:\n",
    "            return None\n",
    "        \n",
    "        possible_assoc = []\n",
    "\n",
    "        if floating_tone and valid_connected: # if there are floating tones, we want it to be connected to the valida connected syllabl\n",
    "            max_valid_syl = max(s for t,m,s in valid_connected)\n",
    "            valid_syl_weight = [m for (t, m, s) in valid_connected if s == max_valid_syl][0]\n",
    "\n",
    "            floating_tone_to_valid_syl = self.assoc[:]\n",
    "\n",
    "            for i, (t,m,s) in enumerate(floating_tone_to_valid_syl):\n",
    "                if t == floating_tone:\n",
    "                    floating_tone_to_valid_syl[i] = (t, valid_syl_weight,max_valid_syl)\n",
    "            possible_assoc.append(floating_tone_to_valid_syl)\n",
    "        \n",
    "        if floating_syl and valid_connected:\n",
    "            \n",
    "            max_valid_tone = max(t for t,m,s in valid_connected)\n",
    "            floating_syl_to_valid_tone = self.assoc[:]\n",
    "            for i, (t,m,s) in enumerate(floating_syl_to_valid_tone):\n",
    "                if s == floating_syl:\n",
    "                    floating_syl_to_valid_tone[i] == (max_valid_tone,m,s)\n",
    "            possible_assoc.append(floating_syl_to_valid_tone)\n",
    "                    \n",
    "        if floating_tone and floating_syl:\n",
    "            \n",
    "            float_tone_to_float_syl = [tup for tup in self.assoc if tup[0] != floating_tone] # Remove tuple by condition\n",
    "            for i, (t, m, s) in enumerate(float_tone_to_float_syl):\n",
    "                if s == floating_syl:\n",
    "                    float_tone_to_float_syl[i] = (floating_tone, m, s)  # Append instead of incorrect indexing\n",
    "            possible_assoc.append(float_tone_to_float_syl)\n",
    "        return possible_assoc\n",
    "\n",
    "\n",
    "    \n",
    "    def next_ar(self):\n",
    "        next_ar = [self.add_syl(i) for i in range(1,max_syl_weight+1)]\n",
    "        next_ar.extend(self.add_tone())\n",
    "        # next_ar = []\n",
    "        if self.add_assoc():\n",
    "            for i in self.add_assoc():\n",
    "                next_ar.append(Autorep('',self.ocp_mel, i))\n",
    "                \n",
    "        return next_ar\n",
    "\n",
    "\n",
    "    def info(self):\n",
    "        return Autorep(ocp_mel = self.ocp_mel, assoc = self.assoc)\n",
    "\n",
    "    def show(self):\n",
    "        print(self.ocp_mel,self.assoc) \n",
    "\n",
    "    def fully_spec(self):\n",
    "        return all(t is not None and m is not None and s is not None for t, m, s in self.assoc)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.ocp_mel == other.ocp_mel and set(self.assoc) == set(other.assoc)\n",
    "\n",
    "    def __hash__(self):\n",
    "        # Define a hash based on the ocp_mel attribute\n",
    "        return hash(self.ocp_mel)\n",
    "        \n",
    "       \n",
    "    def t_factor(self):\n",
    "        tone_num = len(self.ocp_mel)\n",
    "        return tone_num\n",
    "    \n",
    "    def s_factor(self):\n",
    "        syl_num = max([k for _,_,k in self.assoc if k is not None], default=0)\n",
    "        return syl_num     \n",
    "\n",
    "    def k_factor(self):\n",
    "        return self.t_factor() + self.s_factor()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # This will be used when the object is inside a list\n",
    "        return f\"{self.ocp_mel} , {self.assoc}\"\n",
    "\n",
    "    def draw(self, name=\"\"):\n",
    "        \n",
    "        drawing = self.assoc[:]\n",
    "        \n",
    "        for i, tup in enumerate(self.assoc, start=1):  # Assign sequential mora index\n",
    "            if tup[1] is not None or tup[2] is not None:  # Check valid mora assignment\n",
    "                drawing[i - 1] = (tup[0], i, tup[2])  # Create a new tuple with updated mora\n",
    "\n",
    "\n",
    "        if not name:\n",
    "            file_path = os.path.join(\"new_cons\", f\"{self.word}\" if self.word else f\"{self.ocp_mel}\")\n",
    "        else:\n",
    "            file_path = os.path.join(\"new_cons\", f\"{name}\")\n",
    "\n",
    "        d = graphviz.Digraph(filename=file_path, format='png')\n",
    "\n",
    "        # Set global spacing parameters\n",
    "        d.attr(nodesep=\"0.01\", ranksep=\"0.1\")\n",
    "\n",
    "        # Melody nodes\n",
    "        with d.subgraph() as s1:\n",
    "            s1.attr(rank='source', rankdir='LR', nodesep=\"0.01\")\n",
    "            for i, t in enumerate(self.ocp_mel):\n",
    "                s1.node(f'Mel_{i+1}', label=t, shape='plaintext')\n",
    "        \n",
    "        if not self.moralist:\n",
    "            return s1\n",
    "            \n",
    "        # Sigma (syllable) nodes\n",
    "        with d.subgraph() as s2:\n",
    "            # s2.attr(rank='same', rankdir='R', nodesep=\"0.01\")\n",
    "            for j in range(1,self.get_max('s')+1):\n",
    "                s2.node(f'Syl_{j}', label='σ', shape='plaintext')\n",
    "        \n",
    "\n",
    "        for t,m,s in drawing:\n",
    "            # d.node(f'Mora_{mora_index}', label='μ', shape='plaintext')\n",
    "            # d.edge(f'Mora_{mora_index}', f'Syl_{j}', dir='none')\n",
    "            if m is not None and m == 1:\n",
    "                d.node(f'Syl_{j}', label='σ_L', shape='plaintext')\n",
    "            if m is not None and m == 2:\n",
    "                d.node(f'Syl_{j}', label='σ_H', shape='plaintext')\n",
    "                # d.edge(f'Mora_{m}',f'Syl_{s}', dir='none')\n",
    "                \n",
    "                if t is not None:\n",
    "                    d.edge(f'Mel_{t}',  f'Mora_{m}', dir='none')\n",
    "                \n",
    "                \n",
    "        \n",
    "                \n",
    "           \n",
    "      \n",
    "        print(self.word,self.assoc)\n",
    "        return display(d)\n",
    "    \n",
    "    def count_full_tuples(autorep_obj):\n",
    "        \"\"\"\n",
    "        Counts the number of tuples in autorep_obj where all three positions are non-None.\n",
    "        \n",
    "        :param autorep_obj: An instance of Autorep with a list of tuples.\n",
    "        :return: The count of tuples with all three values present.\n",
    "        \"\"\"\n",
    "        return sum(1 for tup in autorep_obj.assoc if all(val is not None for val in tup))\n",
    "\n",
    "\n",
    "    def tone_syl_list(self):\n",
    "        tone_syl_list = []\n",
    "        prev_max = 0  # Store max(tup[2]) of previous tone index\n",
    "        \n",
    "        if self.ocp_mel:\n",
    "            for i in range(self.get_max('t')):  # Loop through tone indices\n",
    "                # Extract `tup[2]` values where `tup[0] == i+1` and `tup[2] is not None`\n",
    "                syl_values = [tup[2] for tup in self.assoc if tup[0] == i + 1 and tup[2] is not None]\n",
    "                \n",
    "                # Compute max(tup[2]) for current tone index\n",
    "                current_max = max(syl_values) if syl_values else prev_max  \n",
    "                \n",
    "                # Compute span: difference from previous max\n",
    "                i_tone_syl = current_max - prev_max if current_max!= prev_max else 1\n",
    "                tone_syl_list.append(i_tone_syl)\n",
    "                \n",
    "                # Update prev_max for next iteration\n",
    "                prev_max = current_max  \n",
    "        \n",
    "        return tone_syl_list\n",
    "    \n",
    "    def syl_tone_list(self):\n",
    "        syl_tone_list = []\n",
    "        prev_max = 0\n",
    "        if self.get_max('s'):\n",
    "             for i in range(self.get_max('s')): \n",
    "                tone_values = [tup[0] for tup in self.assoc if tup[2] == i + 1 and tup[0] is not None]\n",
    "                \n",
    "                # Compute max(tup[2]) for current tone index\n",
    "                current_max = max(tone_values) if tone_values else prev_max  \n",
    "                \n",
    "                # Compute span: difference from previous max\n",
    "                i_syl_tone = current_max - prev_max if current_max!= prev_max else 1\n",
    "                syl_tone_list.append(i_syl_tone)\n",
    "                \n",
    "                # Update prev_max for next iteration\n",
    "                prev_max = current_max  \n",
    "        return syl_tone_list\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 541 words, found 63 ASRs in /Users/hanli/Documents/GitHub/AR_learning/hausa_syllabfied.txt\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "def convert_to_ar(filename):\n",
    "    \"\"\"\n",
    "    Convert a text file to a list of distinct Autorep objects.\n",
    "    \"\"\"\n",
    "    autorep_list = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            asr = Autorep(line.strip())\n",
    "            autorep_list.append(asr)\n",
    "\n",
    "    autoset = set()  # Use a set to avoid duplicate elements\n",
    "    unique_autoreps = []\n",
    "\n",
    "    for i in autorep_list:\n",
    "        info = i.info()\n",
    "        if info not in autoset:\n",
    "            # print(f\"Word: {i.word}\")  # Print the word separately\n",
    "            # pprint.pprint(i)  # Pretty-print the Autorep object\n",
    "            unique_autoreps.append(i)\n",
    "            autoset.add(info)  # Add only the info to the set\n",
    "\n",
    "    print(f'Processed {len(autorep_list)} words, found {len(unique_autoreps)} ASRs in {filename}')\n",
    "    return unique_autoreps\n",
    "\n",
    "# Run the function\n",
    "autolist = convert_to_ar(\"/Users/hanli/Documents/GitHub/AR_learning/hausa_syllabfied.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6 constraints\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ , [(None, 3, 1)],\n",
       "  , [(None, 1, 1), (None, 2, 2), (None, 2, 3), (None, 1, 4)],\n",
       "  , [(None, 2, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4)],\n",
       "  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4), (None, 1, 5)],\n",
       "  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 2, 4), (None, 1, 5)],\n",
       "  , [(None, 1, 1), (None, 1, 2), (None, 2, 3), (None, 1, 4), (None, 1, 5)]]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "def check_from_data(ar, positive_data, find = False):\n",
    "    # Iterate over each autorep in the positive data\n",
    "    for i in positive_data:\n",
    "        # Check if the given autorep is a sub_structure of the current autorep in the loop\n",
    "        if i.check_contain(ar):\n",
    "            if find == True:\n",
    "               print(i.word)\n",
    "            # i.draw()\n",
    "            # If it is, return True immediately\n",
    "            return True\n",
    "    # If the loop completes without finding any match, return False\n",
    "    return False        \n",
    "\n",
    "\n",
    "def check_from_grammar(ar, grammar):\n",
    "    # Iterate over each autorep in the positive data\n",
    "   for i in grammar:\n",
    "      # Check if the given autorep contains the structure in the grammar\n",
    "      if ar.check_contain(i):\n",
    "         # If it is, then the grammar check fails\n",
    "         return False\n",
    "   # If the loop completes without finding any match, return True\n",
    "   return True\n",
    "\n",
    "def bufia_syl(D, t = 2, s = 2, m = 5):\n",
    "   D = autolist\n",
    "   t_threshold = t\n",
    "   s_threshold = s\n",
    "   \n",
    "\n",
    "   Q = Queue()\n",
    "   s0 = Autorep()  \n",
    "   V = []\n",
    "   G_syl = []\n",
    "   Q.put(s0)\n",
    "   max_syl_weight = 3\n",
    "   \n",
    "   # checking syllable weight\n",
    "    \n",
    "            \n",
    "   while not Q.empty():\n",
    "      \n",
    "      s = Q.get()\n",
    "      V.append(s)\n",
    "      if check_from_data(s, D):\n",
    "         # print(f'🥰{s} in data')\n",
    "         mora = max_syl_weight \n",
    "         for i in range(1,mora+1):\n",
    "            i = s.add_syl(i)\n",
    "            # print(f'checking the next ar of {s}: {i}')\n",
    "            if i not in V and i not in G_syl and check_from_grammar(i, G_syl):\n",
    "               # print(f'put {i} in Q')\n",
    "               Q.put(i)\n",
    "      else:\n",
    "         # print(f'❌{s}not in Data')\n",
    "         # print(f'❓needed to be in Grammar:{check_from_grammar(s,G_syl)}')\n",
    "         if s not in G_syl and check_from_grammar(s,G_syl):\n",
    "            # s.draw()\n",
    "            # print(f'❌❌{s} add in G')\n",
    "            if s == Autorep('','',[(None, 3, 1)]):\n",
    "               max_syl_weight = 2\n",
    "            if s == Autorep('','',[(None, 2, 1)]):\n",
    "               max_syl_weight = 1\n",
    "            G_syl.append(s)\n",
    "   \n",
    "\n",
    "   print(f'found {len(G_syl)} constraints')\n",
    "   return G_syl \n",
    "bufia_syl(autolist,10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Autorep(\"\",\"\",[(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4), (None, 1, 5)])\n",
    "b = Autorep(\"\",\"\",[(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4), (None, 2, 5)])\n",
    "a.check_contain(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6 constraints\n",
      "found 12 constraints\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ , [(None, 3, 1)],\n",
       "  , [(None, 1, 1), (None, 2, 2), (None, 2, 3), (None, 1, 4)],\n",
       "  , [(None, 2, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4)],\n",
       "  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4), (None, 1, 5)],\n",
       "  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 2, 4), (None, 1, 5)],\n",
       "  , [(None, 1, 1), (None, 1, 2), (None, 2, 3), (None, 1, 4), (None, 1, 5)],\n",
       " HL , [(1, 1, 1), (2, 1, 1)],\n",
       " LH , [(1, 1, 1), (2, 1, 1)],\n",
       " LH , [(1, 2, 1), (2, 2, 1)],\n",
       " HL , [(1, 2, 1), (None, 1, 2), (2, 2, 1)],\n",
       " HL , [(1, 2, 1), (None, 2, 2), (2, 2, 1)],\n",
       " LHL , [(1, 2, 1), (2, 2, 2), (3, 2, 2)]]"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bufia(D, t = 2, s = 2, m = 5):\n",
    "   D = autolist\n",
    "   t_threshold = t\n",
    "   s_threshold = s\n",
    "   \n",
    "\n",
    "   Q = Queue()\n",
    "   s0 = Autorep()  \n",
    "   V = []\n",
    "   G = bufia_syl(D)\n",
    "   Q.put(s0)\n",
    "\n",
    "   while not Q.empty():\n",
    "      s = Q.get()\n",
    "   \n",
    "      V.append(s)\n",
    "      # print(check_from_data(s, D))\n",
    "      if check_from_data(s, D):\n",
    "         S = s.next_ar()\n",
    "         # print(S)\n",
    "         for i in S:\n",
    "            # print(i)\n",
    "            if i not in V and i not in G and check_from_grammar(i, G) and i.t_factor() <= t_threshold and i.s_factor() <= s_threshold and i.get_max('m') <= m:\n",
    "               Q.put(i)\n",
    "      else:\n",
    "         if s not in G and check_from_grammar(s,G):\n",
    "            # s.draw()\n",
    "            G.append(s)\n",
    "               \n",
    "\n",
    "\n",
    "   print(f'found {len(G)} constraints')\n",
    "   return G \n",
    "\n",
    "\n",
    "bufia(autolist,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty = Autorep()\n",
    "empty.next_ar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking the next ar of  , []\n",
      "checking the next ar of  , []\n",
      "checking the next ar of  , []\n",
      "checking the next ar of  , [(None, 1, 1)]\n",
      "checking the next ar of  , [(None, 1, 1)]\n",
      "checking the next ar of  , [(None, 1, 1)]\n",
      "checking the next ar of  , [(None, 2, 1)]\n",
      "checking the next ar of  , [(None, 2, 1)]\n",
      "checking the next ar of  , [(None, 2, 1)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 2, 2)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 2, 2)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 2, 2)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 1, 2), (None, 2, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 2, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 2, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 2, 1), (None, 2, 2), (None, 1, 3)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 2, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 2, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 2, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 2, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 2, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 1, 2), (None, 2, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 1, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 1, 3), (None, 1, 4)]\n",
      "checking the next ar of  , [(None, 1, 1), (None, 2, 2), (None, 1, 3), (None, 1, 4)]\n",
      "found 3 constraints\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ , [(None, 3, 1)],\n",
       "  , [(None, 2, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4)],\n",
       "  , [(None, 1, 1), (None, 1, 2), (None, 1, 3), (None, 1, 4), (None, 1, 5)]]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mù.tûm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_from_data(mutum,autolist,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fall_mono = Autorep(\"\",\"HL\",[(1,1,1),(2,1,1)])\n",
    "\n",
    "# check_from_data(fall_mono,autolist,True)\n",
    "# fall.check_contain(fall_mono)\n",
    "\n",
    "# mutum.check_contain(fall_mono)\n",
    "\n",
    "mutum = Autorep(\"mù.tûm\")\n",
    "mutum\n",
    "\n",
    "mutum.check_contain(fall_mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HL , [(1, 2, 1), (2, 1, 2), (2, 2, 3)]\n",
      "tone-syl list for containee [1, 1]\n",
      "tone-syl list for container [1, 2]\n",
      "syl_tone_listfor containee [1, 1]\n",
      "syl_tone_list for container [1, 1, 1]\n",
      "in containee, the 1 syllable has weight of 1\n",
      "in res, the 1 syllable has weight of 2\n",
      "gáa.jì.màa.rée contains dú.hùu: False\n",
      "gáa.jì.màa.rée contains fà.dá.màa: False\n",
      "HL , [(1, 1, 1), (2, 2, 2)]\n",
      "tone-syl list for containee [1, 1]\n",
      "tone-syl list for container [1, 1]\n",
      "syl_tone_listfor containee [1, 1]\n",
      "syl_tone_list for container [1, 1]\n",
      "in containee, the 1 syllable has weight of 1\n",
      "in res, the 1 syllable has weight of 1\n",
      "in containee, the 2 syllable has weight of 2\n",
      "in res, the 2 syllable has weight of 2\n",
      "fà.dá.màa contains dú.hùu: True\n",
      "fà.dá.màa contains gáa.jì.màa.rée: False\n",
      "dú.hùu contains fà.dá.màa: False\n",
      "dú.hùu contains gáa.jì.màa.rée: False\n",
      "d contains fà.dá.màa: True\n",
      "e contains fà.dá.màa: True\n",
      "Rising contains float_rising: True\n",
      "Superheavy contains heavy: True\n",
      "Rising contains heavy: True\n",
      "Fall contains float H: True\n",
      "Fall contains monosyllabic float H: True\n",
      "LH , [(1, 1, 1), (2, None, None), (None, 2, 1), (None, 1, 2), (None, 1, 3)]\n",
      "tone-syl list for containee [1, 1]\n",
      "tone-syl list for container [1, 1]\n",
      "syl_tone_listfor containee [1]\n",
      "syl_tone_list for container [1, 1, 1]\n",
      "in containee, the 1 syllable has weight of 1\n",
      "in res, the 1 syllable has weight of 2\n",
      "wrong1 contains wrong2: False\n",
      "HL , [(1, 1, 1), (1, 2, 1), (2, 1, 2), (2, 1, 3)]\n",
      "tone-syl list for containee [1, 2]\n",
      "tone-syl list for container [1, 2]\n",
      "syl_tone_listfor containee [1, 1, 1]\n",
      "syl_tone_list for container [1, 1, 1]\n",
      "in containee, the 1 syllable has weight of 1\n",
      "in res, the 1 syllable has weight of 2\n",
      "wrong3 contains wrong2: False\n",
      "wrong5 contains wrong4: True\n",
      "H , [(1, 2, 1), (1, 1, 2), (1, 2, 2), (1, 1, 3)]\n",
      "tone-syl list for containee [2]\n",
      "tone-syl list for container [3]\n",
      "syl_tone_listfor containee [1, 1]\n",
      "syl_tone_list for container [1, 1, 1]\n",
      "in containee, the 1 syllable has weight of 2\n",
      "in res, the 1 syllable has weight of 2\n",
      "in containee, the 2 syllable has weight of 1\n",
      "in res, the 2 syllable has weight of 2\n",
      "wrong6 contains wrong7: False\n",
      "LH , [(1, 2, 1), (2, 2, 2)]\n",
      "tone-syl list for containee [1, 1]\n",
      "tone-syl list for container [1, 1]\n",
      "syl_tone_listfor containee [2]\n",
      "syl_tone_list for container [1, 1]\n",
      "kùu.ráa contains rising: False\n",
      "L , [(1, 1, 1), (1, 2, 1), (None, 1, 2), (None, 2, 2), (None, 1, 3), (None, 2, 3)]\n",
      "tone-syl list for containee [1]\n",
      "tone-syl list for container [1]\n",
      "syl_tone_listfor containee [1, 1, 1]\n",
      "syl_tone_list for container [1, 1, 1]\n",
      "in containee, the 1 syllable has weight of 1\n",
      "in res, the 1 syllable has weight of 2\n",
      "wrong13 contains wrong14: False\n",
      "H , [(1, 4, 1)]\n",
      "tone-syl list for containee [1]\n",
      "tone-syl list for container [1]\n",
      "syl_tone_listfor containee [1]\n",
      "syl_tone_list for container [1]\n",
      "in containee, the 1 syllable has weight of 3\n",
      "in res, the 1 syllable has weight of 4\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "# ============================\n",
    "# Creating Autorep Objects\n",
    "# ============================\n",
    "a = Autorep(\"dú.hùu\")  \n",
    "b = Autorep(\"fà.dá.màa\")  \n",
    "c = Autorep(\"gáa.jì.màa.rée\") \n",
    "\n",
    "# ============================\n",
    "# Testing Containment\n",
    "# ============================\n",
    "print(f\"gáa.jì.màa.rée contains dú.hùu: {c.check_contain(a)}\")  # Expect False\n",
    "print(f\"gáa.jì.màa.rée contains fà.dá.màa: {c.check_contain(b)}\")  # Expect False\n",
    "print(f\"fà.dá.màa contains dú.hùu: {b.check_contain(a)}\")  # Expect True\n",
    "print(f\"fà.dá.màa contains gáa.jì.màa.rée: {b.check_contain(c)}\")  # Expect False\n",
    "print(f\"dú.hùu contains fà.dá.màa: {a.check_contain(b)}\")  # Expect False\n",
    "print(f\"dú.hùu contains gáa.jì.màa.rée: {a.check_contain(c)}\")  # Expect False\n",
    "\n",
    "# ============================\n",
    "# Modifying and Testing b\n",
    "# ============================\n",
    "d = b.add_tone()[0]\n",
    "e = b.add_syl(2)\n",
    "\n",
    "print(f\"d contains fà.dá.màa: {d.check_contain(b)}\")  # Expect True\n",
    "print(f\"e contains fà.dá.màa: {e.check_contain(b)}\")  # Expect True\n",
    "\n",
    "# ============================\n",
    "# Testing Containment with Variations\n",
    "# ============================\n",
    "float_rising = Autorep(ocp_mel=\"LH\", assoc=[(1, None, None), (2, None, None)])\n",
    "rising = Autorep(ocp_mel=\"LH\", assoc=[(2, 2, 1), (1, 1, 1)])\n",
    "print(f\"Rising contains float_rising: {rising.check_contain(float_rising)}\")  # Expect True\n",
    "\n",
    "heavy = Autorep(assoc=[(None, 1, 1), (None, 2, 1)])\n",
    "superheavy = Autorep(\"\", \"\", [(None, 1, 1), (None, 2, 1), (None, 3, 1)])\n",
    "print(f\"Superheavy contains heavy: {superheavy.check_contain(heavy)}\")  # Expect True\n",
    "\n",
    "rising = Autorep(ocp_mel=\"LH\", assoc=[(1, 1, 1), (2, 2, 1)])\n",
    "fall = Autorep(ocp_mel=\"HL\", assoc=[(1, 1, 1), (2, 2, 1)])\n",
    "print(f\"Rising contains heavy: {rising.check_contain(heavy)}\")  # Expect False\n",
    "\n",
    "floatH = Autorep(\"\", \"H\", assoc=[(1, None, None)])\n",
    "print(f\"Fall contains float H: {fall.check_contain(floatH)}\")\n",
    "\n",
    "floatH_mono = Autorep(ocp_mel=\"H\", assoc=[(1, None, None), (None, 1, 1)])\n",
    "print(f\"Fall contains monosyllabic float H: {fall.check_contain(floatH_mono)}\")\n",
    "\n",
    "# ============================\n",
    "# Testing Various Wrong Cases\n",
    "# ============================\n",
    "wrong1 = Autorep(\"\", \"LH\", [(1, 1, 1), (None, 2, 1), (2, None, None), (None, 1, 2), (None, 1, 3)])\n",
    "wrong2 = Autorep(\"\", \"LH\", [(1, 1, 1), (2, None, None)])\n",
    "print(f\"wrong1 contains wrong2: {wrong1.check_contain(wrong2)}\")  # Expect True\n",
    "\n",
    "wrong3 = Autorep(\"\", \"HL\", [(1, 1, 1), (1, 2, 1), (2, 1, 2), (2, 1, 3)])\n",
    "wrong2 = Autorep(\"\", \"HL\", [(1, 1, 1), (2, 1, 2), (2, 1, 3)])\n",
    "print(f\"wrong3 contains wrong2: {wrong3.check_contain(wrong2)}\")  # Expect True\n",
    "\n",
    "wrong4 = Autorep(\"\", \"LH\", [(1, None, None), (None, 1, 1), (2, None, None)])\n",
    "wrong5 = Autorep(\"\", \"LH\", [(1, 1, 1), (2, None, None)])\n",
    "print(f\"wrong5 contains wrong4: {wrong5.check_contain(wrong4)}\")  # Expect True\n",
    "\n",
    "wrong6 = Autorep(\"\", \"LH\", [(1, 1, 1), (2, 2, 1), (2, 1, 2), (2, 2, 2), (2, 1, 3)])\n",
    "wrong7 = Autorep(\"\", \"H\", [(1, 1, 1), (1, 2, 1), (1, 1, 2)])\n",
    "print(f\"wrong6 contains wrong7: {wrong6.check_contain(wrong7)}\")  # Expect True\n",
    "\n",
    "kuuraa = Autorep(\"kùu.ráa\")\n",
    "print(f\"kùu.ráa contains rising: {kuuraa.check_contain(rising)}\")  # Expect False\n",
    "\n",
    "wrong13 = Autorep(\"\", \"L\", [(1, 1, 1), (1, 2, 1), (None, 1, 2), (None, 2, 2), (None, 1, 3), (None, 2, 3)])\n",
    "wrong14 = Autorep(\"\", \"L\", [(1, 1, 1), (None, 1, 2), (None, 2, 2), (None, 1, 3), (None, 2, 3)])\n",
    "print(f\"wrong13 contains wrong14: {wrong13.check_contain(wrong14)}\")  # Expect True\n",
    "\n",
    "three_mu_H = Autorep(\"\",\"H\",assoc= [(1, 3, 1)])\n",
    "four_mu_H = Autorep(\"\",\"H\",assoc=  [(1, 4, 1)])\n",
    "print(four_mu_H.check_contain(three_mu_H))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall contains float H: True\n",
      "Fall contains monosyllabic float H: True\n"
     ]
    }
   ],
   "source": [
    "floatH = Autorep(\"\", \"H\", assoc=[(1, None, None)])\n",
    "print(f\"Fall contains float H: {fall.check_contain(floatH)}\")\n",
    "\n",
    "floatH_mono = Autorep(ocp_mel=\"H\", assoc=[(1, None, None), (None, 1, 1)])\n",
    "print(f\"Fall contains monosyllabic float H: {fall.check_contain(floatH_mono)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rising contains heavy: True\n"
     ]
    }
   ],
   "source": [
    "heavy = Autorep(assoc=[(None, 1, 1), (None, 2, 1)])\n",
    "superheavy = Autorep(\"\", \"\", [(None, 1, 1), (None, 2, 1), (None, 3, 1)])\n",
    "# print(f\"Superheavy contains heavy: {superheavy.check_contain(heavy)}\")  # Expect True\n",
    "\n",
    "rising = Autorep(ocp_mel=\"LH\", assoc=[(1, 1, 1), (2, 2, 1)])\n",
    "fall = Autorep(ocp_mel=\"HL\", assoc=[(1, 1, 1), (2, 2, 1)])\n",
    "print(f\"Rising contains heavy: {rising.check_contain(heavy)}\")  # Expect False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  # Needed to convert string to list\n",
    "\n",
    "\n",
    "# Convert 'AR' column from string to list of tuples\n",
    "\n",
    "\n",
    "# Now iterate over the rows\n",
    "autolist_jp = []\n",
    "for _, row in jp.iterrows():\n",
    "    asr = Autorep(ocp_mel=row[\"tone\"], assoc=row[\"AR\"])  # 'AR' is now a proper list\n",
    "    autolist_jp.append(asr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "jp = pd.read_csv(\"~/Desktop/jp.csv\")\n",
    "\n",
    "jp\n",
    "jp[\"AR\"] = jp[\"AR\"].apply(ast.literal_eval)\n",
    "\n",
    "autolist_jp = []\n",
    "for _, row in jp.iterrows():\n",
    "    asr = Autorep(ocp_mel=row[\"tone\"], assoc=row[\"AR\"])  # 'AR' is now a proper list\n",
    "    autolist_jp.append(asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_ar(filename):\n",
    "    \"\"\"\n",
    "    Convert a text file to a list of distinct Autorep objects.\n",
    "    \"\"\"\n",
    "    autorep_list =[]\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            asr = Autorep(line.strip())\n",
    "            # print(asr.assoc)\n",
    "            autorep_list.append(asr)\n",
    "            #if Autorep(line.strip()).check_contain(LL):\n",
    "             #  print(line)\n",
    "    autoset = []\n",
    "\n",
    "    for i in autorep_list:\n",
    "        if i.info() not in autoset:\n",
    "            autoset.append(i)\n",
    "    print(f'processed {len(autorep_list)} words, found {len(autoset)} ASRs in {file.name}')\n",
    "    return autoset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Desktop/jp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert_to_ar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/Desktop/jp.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[131], line 6\u001b[0m, in \u001b[0;36mconvert_to_ar\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mConvert a text file to a list of distinct Autorep objects.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m autorep_list \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m         asr \u001b[38;5;241m=\u001b[39m Autorep(line\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Desktop/jp.csv'"
     ]
    }
   ],
   "source": [
    "convert_to_ar(\"~/Desktop/jp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
